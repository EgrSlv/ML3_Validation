{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228f2262",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Chapter V. Task**\n",
    "\n",
    "We will continue our training with a problem from Kaggle.com. \n",
    "In this chapter, we will implement all the validation schemes, some hyperparameter tuning methods, and feature selection methods described above. Measure quality metrics on training and test samples. Will detect overfitted models and regularize them. And dive deeper with native model estimation and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f5fef",
   "metadata": {},
   "source": [
    "### 1. **Answer the questions from the introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c16cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### a. What is leave-one-out? Provide limitations and strengths.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd72f3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Leave-One-Out кросс валидация - это частный случай k-fold Cross Validation, где k = n. Данные размерности 10 делятся на обучающую и тестовую выборки по принципу 9 обучающей и 1 тестовая на каждой итерации для всех данных. При этом,каждый набор данных __ровно один раз__ становится тестовым (см. пример кода.). Другими словами, для **_n_** образцов мы имеем **_n-1_** тренировочных данных и **1** тестовый набор.\n",
    "Достоинства:\n",
    " * максимальное использование данных для обучения\n",
    " * не зависит от случайного разбиения\n",
    " * эффективен только на маленьких наборах данных\n",
    "\n",
    "Недостатки:\n",
    " * высокая стоимость вычислительных операций для больших выборок данных - O(n)\n",
    " * высокая дисперсия оценки\n",
    " * может недооценивать ошибку на новых данных\n",
    "\n",
    "```Python\n",
    ">>> from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    ">>> X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    ">>> loo = LeaveOneOut()\n",
    ">>> for train, test in loo.split(X):\n",
    "...     print(\"%s %s\" % (train, test))\n",
    "\n",
    "[1 2 3 4 5 6 7 8 9] [0]\n",
    "[0 2 3 4 5 6 7 8 9] [1]\n",
    "[0 1 3 4 5 6 7 8 9] [2]\n",
    "[0 1 2 4 5 6 7 8 9] [3]\n",
    "[0 1 2 3 5 6 7 8 9] [4]\n",
    "[0 1 2 3 4 6 7 8 9] [5]\n",
    "[0 1 2 3 4 5 7 8 9] [6]\n",
    "[0 1 2 3 4 5 6 8 9] [7]\n",
    "[0 1 2 3 4 5 6 7 9] [8]\n",
    "[0 1 2 3 4 5 6 7 8] [9]\n",
    "```\n",
    "\n",
    "Источник: https://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb16e96",
   "metadata": {},
   "source": [
    "#### b. How do Grid Search, Randomized Grid Search, and Bayesian optimization work?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7c9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {1, 2, 3, 4, 5}\n",
    "B = {6, 7, 8, 9}\n",
    "C = {10, 11, 12}\n",
    "\n",
    "def cartesian_product(*sets):\n",
    "    \"\"\"\n",
    "    Декартово произведение множеств\n",
    "    \"\"\"\n",
    "    result = [[]]\n",
    "    for pool in sets:\n",
    "        result = [x + [y] for x in result for y in pool]\n",
    "    return [tuple(item) for item in result]\n",
    "\n",
    "decart = cartesian_product(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342136f",
   "metadata": {},
   "source": [
    "> Этапы работы поиска наилучших гиперпараметров через Grid Search:\n",
    " 1. **Определение пространства внешних гиперпараметров**. На первом этапе необходимо обозначить какие гиперпараметры мы ищем. Параметр должен содержать множество значений, по которым мы будем проводить поиск. Параметров может быть несколько. Несколько параметров собираются в единый словарь, где ключи — имена гиперпараметров, а значения — списки возможных значений для каждого параметра.\n",
    " 2. **Создание сетки гиперпараметров**. Создается декартово произведение всех списков значений, получая полный набор всех возможных комбинаций гиперпараметров.\n",
    " 3. **Проход по списку гиперпараметров**. Алгоритм GridSearch последовательно проходит по каждой комбинации гиперпараметров, обучается и оценивается с помощью k-fold CV, вычисляется средняя оценка гиперпараметров по всем фолдам для заданной метрики.\n",
    " 4. **Сохранение и возвращение результата**. По окончании подбора параметров выбирается комбинация с наилучшей средней оценкой. GridSearch возвращает: лучшие гиперпараметры `best_params_`; лучшие оценки `best_score_`; полную информацию по всем комбинациям `cv_results_`.\n",
    "\n",
    "> Randomized Grid Search:\n",
    " * Принцип работы Randomized Grid Search схож с работой Grid Search, но не создает полную сетку всех комбинаций гиперпараметров. Вместо этого поиск происходит случайным образом. Еще одно отличие - это наличие итераций. На каждой итерации `n_iter` случайным образом генерируется комбинация гиперпараметров, происходит обучение модели и оценка её качества с помощью k-fold CV на всех фолдах. Случайный выбор позволяет ускорить процесс подбора гиперпараметров. В таком случае, гиперпараметры следует задавать не только дискретным образом, но и в диапазоне и\\или распределении, например: `stats.uniform(0.1, 10)`, здесь будет создано распределение случайных чисел от 0.1 до 10 + 0.1. Случайное число из данного распределения будет получено внутри GridSearch.\n",
    "\n",
    "> Bayesian optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa1c8d",
   "metadata": {},
   "source": [
    "#### c. Explain classification of feature selection methods. Explain how Pearson and Chi2 work. Explain how Lasso works. Explain what permutation significance is. Become familiar with SHAP.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "095d59ab",
   "metadata": {},
   "source": [
    "### **2. Introduction — do all the preprocessing from the previous lesson**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c3476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daecaf5",
   "metadata": {},
   "source": [
    "#### a. Read all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b5df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../datasets/train.json\", convert_dates=[\"created\"]).reset_index(drop=True)\n",
    "    # pd.read_json(\"../datasets/test.json\") - здесь нет признака 'interest_level'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578bdfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### b. Preprocess the \"Interest Level\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614e5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"interest_level\"] = df[\"interest_level\"].map({\"low\": 0, \"medium\": 1, \"high\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dfd6b",
   "metadata": {},
   "source": [
    "#### c. Create features: 'Elevator', 'HardwoodFloors', 'CatsAllowed', 'DogsAllowed', 'Doorman', 'Dishwasher', 'NoFee', 'LaundryinBuilding', 'FitnessCenter', 'Pre-War', 'LaundryinUnit', 'RoofDeck', 'OutdoorSpace', 'DiningRoom', 'HighSpeedInternet', 'Balcony', 'SwimmingPool', 'LaundryInBuilding', 'NewConstruction', 'Terrace'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebdd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"[\\[\\]\\'\\\"\\s]\")\n",
    "remove_sym = (lambda x: list([re.sub(pattern, '', elem) for elem in x]))\n",
    "df[\"features\"] = df[\"features\"].apply(remove_sym)\n",
    "\n",
    "target = [\"price\"]\n",
    "rooms = [\"bedrooms\", \"bathrooms\"]\n",
    "features = ['Elevator', 'HardwoodFloors', 'CatsAllowed', 'DogsAllowed',\n",
    "            'Doorman', 'Dishwasher', 'NoFee', 'LaundryinBuilding',\n",
    "            'FitnessCenter', 'Pre-War', 'LaundryinUnit', 'RoofDeck',\n",
    "            'OutdoorSpace', 'DiningRoom', 'HighSpeedInternet', 'Balcony',\n",
    "            'SwimmingPool', 'LaundryInBuilding', 'NewConstruction', 'Terrace',]\n",
    "for feature in features:\n",
    "    df[feature] = (\n",
    "        df[\"features\"]\n",
    "        .apply(lambda x: 1 if feature in x else 0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25651014",
   "metadata": {},
   "source": [
    "### 3. **Implement the next methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7006ca",
   "metadata": {},
   "source": [
    "#### a. Split data into 2 parts randomly with parameter test_size (ratio from 0 to 1), return training and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b66524c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "# 3.a\n",
    "def my_train_test_split(X, y, test_size=0.25):\n",
    "    n = len(X)\n",
    "    if isinstance(test_size, int):\n",
    "        if test_size <= 0 or test_size >= n:\n",
    "            raise ValueError(\"'test_size' must be between 1 and len(X)-1\")\n",
    "    elif isinstance(test_size, float):\n",
    "        if test_size <= 0.0 or test_size >= 1.0:\n",
    "            raise ValueError(\"'test_size' must be between 0.0 and 1.0\")\n",
    "        test_size = ceil(n * test_size)\n",
    "    else:\n",
    "        raise ValueError(\"'test_size' must be float or int\")\n",
    "\n",
    "    if n != len(y):\n",
    "        raise ValueError(\"Length of 'X' and 'y' must be equal\")\n",
    "\n",
    "    idx = X.index.to_numpy()\n",
    "    train_idxs = idx[test_size:]\n",
    "    test_idxs = idx[:test_size]\n",
    "\n",
    "    return (pd.concat([X.loc[train_idxs], y.loc[train_idxs]], axis=1),\n",
    "            pd.concat([X.loc[test_idxs], y.loc[test_idxs]], axis=1))\n",
    "\n",
    "\n",
    "train, test = my_train_test_split(\n",
    "    df.drop(columns=target),\n",
    "    df[target],\n",
    "    test_size=0.2\n",
    ")\n",
    "display(\n",
    "    train.shape[0] + test.shape[0] == len(df)\n",
    ")\n",
    "\n",
    "train_idx, test_idx = train.index, test.index\n",
    "\n",
    "print(train_idx.intersection(test_idx).empty,\n",
    "      test_idx.intersection(train_idx).empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02c8cb",
   "metadata": {},
   "source": [
    "#### b. Randomly split data into 3 parts with parameters validation_size and test_size, return train, validation and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e14674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n"
     ]
    }
   ],
   "source": [
    "# 3.b\n",
    "def my_train_validation_test_split(X, y, validation_size=0.25, test_size=0.25):\n",
    "    n = len(X)\n",
    "    if isinstance(test_size, int) and isinstance(validation_size, int):\n",
    "        if test_size <= 0 or test_size >= n:\n",
    "            raise ValueError(\"'test_size' must be between 1 and len(X)-1\")\n",
    "        if validation_size <= 0 or validation_size >= n:\n",
    "            raise ValueError(\"'validation_size' must be between 1 and len(X)-1\")\n",
    "    elif isinstance(test_size, float) and isinstance(validation_size, float):\n",
    "        if test_size <= 0.0 or test_size >= 1.0:\n",
    "            raise ValueError(\"'test_size' must be between 0.0 and 1.0\")\n",
    "        if validation_size <= 0.0 or validation_size >= 1.0:\n",
    "            raise ValueError(\"'validation_size' must be between 0.0 and 1.0\")\n",
    "        if test_size + validation_size >= 1:\n",
    "            raise ValueError(\"test_size + validation_size must be < 1\")\n",
    "        test_size = ceil(n * test_size)\n",
    "        validation_size = ceil(n * validation_size)\n",
    "    else:\n",
    "        raise ValueError(\"'test_size'/'validation_size' must be float or int\")\n",
    "\n",
    "    idx = X.index.to_numpy()\n",
    "\n",
    "    if n != len(y) or not np.array_equal(idx, y.index.to_numpy()):\n",
    "        raise ValueError(\"Length of 'X' and 'y' must be equal\")\n",
    "    if test_size + validation_size >= n:\n",
    "        raise ValueError(\"test_size + validation_size must be < len(X)\")\n",
    "\n",
    "    train_idxs = idx[test_size + validation_size:]\n",
    "    test_idxs = idx[:test_size]\n",
    "    validation_idxs = idx[test_size:test_size + validation_size]\n",
    "\n",
    "    return (pd.concat([X.loc[train_idxs], y.loc[train_idxs]], axis=1),\n",
    "            pd.concat([X.loc[validation_idxs], y.loc[validation_idxs]], axis=1),\n",
    "            pd.concat([X.loc[test_idxs], y.loc[test_idxs]], axis=1))\n",
    "\n",
    "\n",
    "train, validation, test = my_train_validation_test_split(\n",
    "    df.drop(columns=target),\n",
    "    df[target],\n",
    "    test_size=0.2,\n",
    "    validation_size=0.2\n",
    ")\n",
    "\n",
    "display(train.shape[0] + validation.shape[0] + test.shape[0] == len(df))\n",
    "\n",
    "train_idx = train.index\n",
    "val_idx = validation.index\n",
    "test_idx = test.index\n",
    "\n",
    "print(train_idx.intersection(val_idx).empty,\n",
    "      train_idx.intersection(test_idx).empty,\n",
    "      val_idx.intersection(test_idx).empty)\n",
    "# train, validation = my_train_test_split(\n",
    "#     train.drop(columns=target),\n",
    "#     train[target],\n",
    "#     test_size=0.25  # 0.2 / (1 - 0.2) = 0.25\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411783f",
   "metadata": {},
   "source": [
    "#### c. Split data into 2 parts with parameter date_split, return train and test samples split by date_split param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11634a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d7713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f5238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90d07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc006c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(X, y, *, test_size=0.25, random_state=None, shuffle=True):\n",
    "    random = np.random.RandomState(random_state)\n",
    "    idx = X.index.to_numpy()\n",
    "\n",
    "    # Проверка типов test_size и преобразование в количество объектов\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = ceil(len(X) * test_size)\n",
    "    elif isinstance(test_size, int):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"test_size must be float or int\")\n",
    "\n",
    "    # Флаг shuffle отвечает за перемешивание выборки перед разбиением\n",
    "    if shuffle:\n",
    "        idx = random.permutation(idx)\n",
    "\n",
    "    test_idxs = idx[:test_size]\n",
    "    train_idxs = idx[test_size:]\n",
    "\n",
    "    return (X.loc[train_idxs], X.loc[test_idxs], y.loc[train_idxs], y.loc[test_idxs])\n",
    "\n",
    "\n",
    "# my_train_test_split(df.drop(columns=target), df[target], test_size=0.2, random_state=21)\n",
    "my_train_X, my_test_X, my_train_y, my_test_y = my_train_test_split(df.drop(columns=target), df[target], test_size=4, random_state=21)\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(df.drop(columns=target), df[target], test_size=4, random_state=21)\n",
    "display(\n",
    "    train_X.equals(my_train_X),\n",
    "    train_y.equals(my_train_y),\n",
    "    test_X.equals(my_test_X),\n",
    "    test_y.equals(my_test_y),\n",
    ") # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71721642",
   "metadata": {},
   "source": [
    "#### d. Split data into 3 parts with parameters validation_date and test_date, return train, validation and test samples split by input params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f57d2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"created\"].dt.year\n",
    "df[\"month\"] = df[\"created\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c448f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "6    17144\n",
       "4    16411\n",
       "5    15797\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"month\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1c7a4",
   "metadata": {},
   "source": [
    "#### e. Make split procedure determenistic. What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaccaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "426b730a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 4. **Implement the next cross-validation methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a89d41",
   "metadata": {},
   "source": [
    "#### a. K-Fold, where k is the input parameter, returns a list of train and test indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9d870",
   "metadata": {},
   "source": [
    "#### b. Grouped K-Fold, where k and group_field are input parameters, returns list of train and test indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b2b68",
   "metadata": {},
   "source": [
    "#### c. Stratified K-fold, where k and stratify_field are input parameters, returns list of train and test indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd2473",
   "metadata": {},
   "source": [
    "#### d. Time series split, where k and date_field are input parameters, returns list of train and test indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7cb6a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 5. **Cross-validation comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046e99f",
   "metadata": {},
   "source": [
    "#### a. Apply all the validation methods implemented above to our dataset. To apply Stratified algorithm you should preprocess target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8a7d1",
   "metadata": {},
   "source": [
    "#### b. Apply the appropriate methods from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df757757",
   "metadata": {},
   "source": [
    "#### c. Compare the resulting feature distributions for the training part of the dataset between sklearn and your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c94ddb",
   "metadata": {},
   "source": [
    "#### d. Compare all validation schemes. Choose the best one. Explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686f2ea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 6. **Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f330f09",
   "metadata": {},
   "source": [
    "#### a. Fit a Lasso regression model with normalized features. Use your method for splitting samples into 3 parts by field created with 60/20/20 ratio — train/validation/test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca071d2",
   "metadata": {},
   "source": [
    "#### b. Sort features by weight coefficients from model, fit model to top 10 features and compare quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1262a5d",
   "metadata": {},
   "source": [
    "#### c. Implement method for simple feature selection by nan-ratio in feature and correlation. Apply this method to feature set and take top 10 features, refit model and measure quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f95f49",
   "metadata": {},
   "source": [
    "#### d. Implement permutation importance method and take top 10 features, refit model and measure quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f72988",
   "metadata": {},
   "source": [
    "#### e. Import Shap and also refit model on top 10 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de62330",
   "metadata": {},
   "source": [
    "#### f. Compare the quality of these methods for different aspects — speed, metrics and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86d86e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 7. **Hyperparameter optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524151e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### a. Implement grid search and random search methods for alpha and l1_ratio for sklearn's ElasticNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5382",
   "metadata": {},
   "source": [
    "#### b. Find the best combination of model hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b375b0",
   "metadata": {},
   "source": [
    "#### c. Fit the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceeecd2",
   "metadata": {},
   "source": [
    "#### d. Import optuna and configure the same experiment with ElasticNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc879c",
   "metadata": {},
   "source": [
    "#### e. Estimate metrics and compare approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb2aec",
   "metadata": {},
   "source": [
    "f. Run optuna on one of the cross-validation schemes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
